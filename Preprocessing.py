import pandas as pd
import numpy as np
from collections import defaultdict, Counter

def grouping_by_outer_ip(data: pd.DataFrame, outer_ip_set: set) -> dict:
    '''
    주어진 외부IP 리스트를 기준으로 그룹핑을 진행하는 함수
    '''
    outer_ip_group = defaultdict(list)

    for idx in data.index:
        row = data.iloc[idx].to_dict()
        if row['Src IP'] in outer_ip_set:                   # Source IP가 외부인 경우
            outer_ip_group[row['Src IP']].append(row)
        elif row['Dst IP'] in outer_ip_set:                 # Destination IP가 외부인 경우p:
            outer_ip_group[row['Dst IP']].append(row)
        else:                                               # 정상 외부IP가 없는 플로우인 경우
            continue

    return outer_ip_group

def get_group_features(data: dict) -> pd.DataFrame:
    # 포함될 특징 리스트
    wanted = ["key_ip", "key_inner_port", "key_outer_port", "outer_port_freq",
        "inner_port_freq", "pst_per_flows", "card_inner_ip", "card_inner_port", "card_outer_port",
        "sum_inner_pkts", "avg_inner_pkts", "std_inner_pkts", "sum_outer_pkts", "avg_outer_pkts", "std_outer_pkts",
        "sum_inner_bytes", "avg_inner_bytes", "std_inner_bytes", "sum_outer_bytes", "avg_outer_bytes", "std_outer_bytes", 
        "sum_dur", "avg_dur", "std_dur"]
    new_vectors = dict()

    for column in wanted:
        new_vectors[column] = []
    
    # 같은 외부 아이피 그룹에 대해서 특징 벡터 추출 작업
    for ip in data:
        inner_port_freq = defaultdict(int)
        outer_port_freq = defaultdict(int)
        card_inner_ip = set()
        inner_pkts = []
        outer_pkts = []
        inner_bytes = []
        outer_bytes = []
        dur = []
        flow_set = set()
        count = len(data)

        for row in data[ip]:
            if row['direct']:   # 출발지가 외부 IP인 경우
                outer_port_freq[row['Src Port']] += 1
                inner_port_freq[row['Dst Port']] += 1
                outer_pkts.append(row['Total Fwd Packet'])
                inner_pkts.append(row['Total Bwd packets'])
                outer_bytes.append(row['Total Length of Fwd Packet'])
                inner_bytes.append(row['Total Length of Bwd Packet'])
                flow_set.add((row['Total Bwd packets'], row['Total Fwd Packet'], row['Total Length of Bwd Packet'], row['Total Length of Fwd Packet']))
                card_inner_ip.add(row['Dst IP'])
            else:                      # 출발지가 내부 IP인 경우
                inner_port_freq[row['Src Port']] += 1
                outer_port_freq[row['Dst Port']] += 1
                inner_pkts.append(row['Total Fwd Packet'])
                outer_pkts.append(row['Total Bwd packets'])
                inner_bytes.append(row['Total Length of Fwd Packet'])
                outer_bytes.append(row['Total Length of Bwd Packet'])
                flow_set.add((row['Total Fwd Packet'], row['Total Bwd packets'], row['Total Length of Fwd Packet'], row['Total Length of Bwd Packet']))
                card_inner_ip.add(row['Src IP'])
            dur.append(row['Flow Duration'])
        
        # 각 column에 들어갈 값들을 추가한다.
        max_inner_port = max(inner_port_freq, key=inner_port_freq.get)
        max_outer_port = max(outer_port_freq, key=outer_port_freq.get)
        new_vectors['key_ip'].append(ip)
        new_vectors['key_inner_port'].append(max_inner_port)
        new_vectors['key_outer_port'].append(max_outer_port)
        new_vectors['outer_port_freq'].append(outer_port_freq[max_outer_port] / count)
        new_vectors['inner_port_freq'].append(inner_port_freq[max_inner_port] / count)
        new_vectors["pst_per_flows"].append(len(flow_set))                              # flow에 대한 해석에 따라 달라짐
        new_vectors['card_inner_ip'].append(len(card_inner_ip))
        new_vectors['card_inner_port'].append(len(inner_port_freq.keys()))
        new_vectors['card_outer_port'].append(len(outer_port_freq.keys()))
        new_vectors['sum_inner_pkts'].append(np.sum(inner_pkts))
        new_vectors['avg_inner_pkts'].append(np.mean(inner_pkts))
        new_vectors['std_inner_pkts'].append(np.std(inner_pkts))
        new_vectors['sum_outer_pkts'].append(np.sum(outer_pkts))
        new_vectors['avg_outer_pkts'].append(np.mean(outer_pkts))
        new_vectors['std_outer_pkts'].append(np.std(outer_pkts))
        new_vectors['sum_inner_bytes'].append(np.sum(inner_bytes))
        new_vectors['avg_inner_bytes'].append(np.mean(inner_bytes))
        new_vectors['std_inner_bytes'].append(np.std(inner_bytes))
        new_vectors['sum_outer_bytes'].append(np.sum(outer_bytes))
        new_vectors['avg_outer_bytes'].append(np.mean(outer_bytes))
        new_vectors['std_outer_bytes'].append(np.std(outer_bytes))
        new_vectors['sum_dur'].append(np.sum(dur))
        new_vectors['avg_dur'].append(np.mean(dur))
        new_vectors['std_dur'].append(np.mean(dur))
    
    result = pd.DataFrame()
    for col in wanted:
        new_column = pd.Series(new_vectors[col], name=col)
        result = pd.concat([result, new_column], axis=1)
    
    return result

# CDF 인코딩 테이블 생성 함수
def create_cdf_encoding_table(data: pd.DataFrame, columns: list) -> dict:
    cdf_table = dict()
    count = data.shape[0]
    for feature in columns:
        cdf_table[feature] = Counter(data[feature])
        cumulate_prob = 0
        for key, value in reversed(cdf_table[feature].items()):
            cumulate_prob += (value / count)
            cdf_table[feature][key] = cumulate_prob
    
    return cdf_table

# cdf encoding
def convert_cdf(data: pd.DataFrame, table: dict, columns: list) -> pd.DataFrame:
    for col in columns:
        for key, value in table[col].items():
            data[col] = data[col].replace(to_replace=key, value=value)
    
    return data

def convert_freq(data: pd.DataFrame, columns: list) -> pd.DataFrame:
    for col in columns:
        freq = Counter(data[col])
        for idx in data.index:
            data.loc[idx, col] = freq[data.loc[idx, col]]
    return data

def convert_cdf_to_estimate(data: pd.DataFrame, table: dict, columns: list) -> pd.DataFrame:
    for col in columns:
        unknown = defaultdict(int)
        for key in list(data[col].unique()):
            if key not in table[col].keys():
                unknown[key] += 1
            else:
                data[col] = data[col].replace(to_replace=key, value=table[col][key])
        
        for key, value in unknown.items():
            data[col] = data[col].replace(to_replace=key, value=table[col][-value])
    
    return data

def min_max_scaling(data: pd.DataFrame):
    normalized_df = (data - data.min()) / (data.max() - data.min())
    return normalized_df